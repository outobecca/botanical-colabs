{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "[//]: # ( Horticultural Data Analysis and Exploration )\n[//]: # ( License: MIT License )\n[//]: # ( Repository: https://github.com/outobecca/botanical-colabs )\n\n# \ud83d\udcca Horticultural Data Analysis & Exploration\n**Version 1.0** | Created: 2025-11-04 | Author: Botanical Colabs Team\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/outobecca/botanical-colabs/blob/main/notebooks/data_analysis_exploration.ipynb)\n\n## \ud83d\udccb Overview\n\n**Purpose:** Load, clean, transform, and explore horticultural datasets to discover patterns and inform cultivation decisions.\n\n**Research Question:** How can we efficiently analyze sensor data, soil tests, and plant measurements to optimize growing conditions?\n\n### \ud83c\udfaf Use Cases\n- Load and clean environmental sensor data (temperature, humidity, light)\n- Analyze soil test results (pH, NPK nutrients, organic matter)\n- Explore plant growth measurements\n- Detect data anomalies\n- Generate summary statistics\n- Create visualizations\n\n### \ud83d\udcca Data Sources\n\n| Type | Format | Examples |\n|------|--------|----------|\n| **Sensors** | CSV, JSON | Temperature, humidity, light |\n| **Soil** | CSV, Excel | pH, NPK, moisture |\n| **Plants** | CSV, JSON | Height, yield, phenotype |\n| **Sample** | Built-in | Generated test data |\n\n### \u26a0\ufe0f Notes\n- Sample data provided for learning\n- Upload CSV, Excel, or JSON files\n- Interactive forms for easy input\n- Export results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Background & Methodology\n\n### Scientific Context\nModern horticulture uses data from:\n- Environmental sensors (IoT devices, weather stations)\n- Soil laboratory analyses\n- Plant measurements (growth, yield)\n\nSystematic analysis helps optimize conditions and improve outcomes.\n\n### Methodology\n1. **Data Loading** - Import from files or generate samples\n2. **Data Cleaning** - Handle missing values and outliers\n3. **Exploratory Analysis** - Statistics and distributions\n4. **Visualization** - Charts and plots\n5. **Export** - Save results\n\n### Expected Outputs\n- Summary statistics\n- Time series plots\n- Distribution histograms\n- Correlation heatmaps\n- Cleaned datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \u2699\ufe0f Step 1: Installation and Configuration\n\nRun the cells below to install libraries and configure your analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Library Installation and Import\n# ============================================================================\n\"\"\"\nInstalls required Python libraries.\nRun this cell first.\n\"\"\"\n\n# Installation\n!pip install -q pandas numpy matplotlib seaborn scipy ipywidgets openpyxl plotly scikit-learn\n\n# Core imports\nfrom typing import Dict, Optional, List, Any, Tuple\nfrom IPython.display import display, Markdown, HTML\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ipywidgets as widgets\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"\u2705 Libraries installed successfully\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Interactive Configuration\n# ============================================================================\n\n# Data source selection (FORM)\nprint(\"\ud83d\udccb SELECT DATA SOURCE:\")\ndata_source_options = {\n    '1': 'Sample Environmental Data (30 days of sensor readings)',\n    '2': 'Sample Soil Analysis Data (50 samples)',\n    '3': 'Sample Plant Growth Data (100 plants)',\n    '4': 'Upload My File (CSV, Excel, or JSON)'\n}\n\nfor key, desc in data_source_options.items():\n    print(f\"  [{key}] {desc}\")\n\nDATA_SOURCE_CHOICE = input(\"",
        "Enter choice (1-4): \").strip() or '1'\n\n# Upload file if needed\nUPLOADED_DATA = None\nif DATA_SOURCE_CHOICE == '4':\n    print(\"",
        "\ud83d\udce4 Upload your file in the next cell using Google Colab's file upload\")\n    from google.colab import files\n    uploaded = files.upload()\n    if uploaded:\n        filename = list(uploaded.keys())[0]\n        print(f\"\u2705 Uploaded: {filename}\")\n        # Load the file\n        if filename.endswith('.csv'):\n            UPLOADED_DATA = pd.read_csv(filename)\n        elif filename.endswith(('.xlsx', '.xls')):\n            UPLOADED_DATA = pd.read_excel(filename)\n        elif filename.endswith('.json'):\n            UPLOADED_DATA = pd.read_json(filename)\n        print(f\"\ud83d\udcca Loaded {len(UPLOADED_DATA)} rows\")\n\n# Outlier handling (FORM)\nprint(\"",
        "\ud83c\udfaf OUTLIER DETECTION:\")\nprint(\"  [1] Remove outliers (Z-score method)\")\nprint(\"  [2] Remove outliers (IQR method)\")\nprint(\"  [3] Keep all data\")\n\nOUTLIER_CHOICE = input(\"Enter choice (1-3): \").strip() or '1'\nREMOVE_OUTLIERS = OUTLIER_CHOICE in ['1', '2']\nOUTLIER_METHOD = 'zscore' if OUTLIER_CHOICE == '1' else 'iqr'\n\nif REMOVE_OUTLIERS:\n    Z_THRESHOLD = float(input(\"Z-score threshold (default 3.0): \").strip() or '3.0')\nelse:\n    Z_THRESHOLD = 3.0\n\nprint(\"",
        "\u2705 Configuration complete!\")\nprint(f\"   Data source: {data_source_options[DATA_SOURCE_CHOICE]}\")\nprint(f\"   Outlier handling: {OUTLIER_METHOD if REMOVE_OUTLIERS else 'None'}\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Step 2: Helper Functions\n\nData processing utilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Helper Functions\n# ============================================================================\n\ndef generate_environmental_data(days=30):\n    \"\"\"Generate sample environmental sensor data.\"\"\"\n    np.random.seed(42)\n    dates = pd.date_range(end=datetime.now(), periods=days*24, freq='H')\n    hours = np.array([d.hour for d in dates])\n    \n    # Realistic patterns\n    temp = 22 + 5 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 1, len(dates))\n    humidity = 60 - 15 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 3, len(dates))\n    light = np.maximum(0, 400 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 30, len(dates)))\n    \n    df = pd.DataFrame({\n        'timestamp': dates,\n        'temperature_c': temp,\n        'humidity_percent': humidity,\n        'light_ppfd': light,\n        'soil_moisture': 65 + np.random.normal(0, 5, len(dates))\n    })\n    \n    # Add anomalies\n    anomalies = np.random.choice(len(df), 5, replace=False)\n    df.loc[anomalies, 'temperature_c'] += np.random.choice([-10, 10], 5)\n    \n    return df\n\ndef generate_soil_data(n=50):\n    \"\"\"Generate sample soil analysis data.\"\"\"\n    np.random.seed(42)\n    return pd.DataFrame({\n        'sample_id': [f'SOIL_{i:03d}' for i in range(1, n+1)],\n        'location': np.random.choice(['Field A', 'Field B', 'Field C'], n),\n        'ph': np.clip(np.random.normal(6.5, 0.5, n), 5.0, 8.0),\n        'nitrogen_ppm': np.clip(np.random.normal(45, 10, n), 0, 100),\n        'phosphorus_ppm': np.clip(np.random.normal(30, 8, n), 0, 80),\n        'potassium_ppm': np.clip(np.random.normal(180, 30, n), 0, 300),\n        'organic_matter_%': np.clip(np.random.normal(4.5, 1.2, n), 1, 10),\n        'date': pd.date_range(end=datetime.now(), periods=n)\n    })\n\ndef generate_plant_data(n=100):\n    \"\"\"Generate sample plant growth data.\"\"\"\n    np.random.seed(42)\n    treatments = ['Control', 'Treatment A', 'Treatment B']\n    df = pd.DataFrame({\n        'plant_id': [f'P{i:04d}' for i in range(1, n+1)],\n        'variety': np.random.choice(['Var1', 'Var2', 'Var3'], n),\n        'treatment': np.random.choice(treatments, n),\n        'height_cm': np.random.normal(45, 12, n),\n        'leaf_count': np.random.poisson(25, n),\n        'yield_g': np.random.normal(125, 30, n),\n        'date': pd.date_range(end=datetime.now(), periods=n)[::-1]\n    })\n    \n    # Treatment effects\n    for treat, factor in {'Control': 1.0, 'Treatment A': 1.15, 'Treatment B': 1.25}.items():\n        mask = df['treatment'] == treat\n        df.loc[mask, ['height_cm', 'yield_g']] *= factor\n    \n    return df.clip(lower=0)\n\ndef detect_outliers(df, column, method='zscore', threshold=3.0):\n    \"\"\"Detect outliers in a column.\"\"\"\n    if method == 'zscore':\n        z = np.abs((df[column] - df[column].mean()) / df[column].std())\n        return z > threshold\n    else:  # IQR\n        Q1, Q3 = df[column].quantile([0.25, 0.75])\n        IQR = Q3 - Q1\n        return (df[column] < Q1 - 1.5*IQR) | (df[column] > Q3 + 1.5*IQR)\n\ndef clean_data(df, remove_outliers=True, method='zscore', threshold=3.0):\n    \"\"\"Clean dataframe.\"\"\"\n    df_clean = df.copy()\n    \n    # Fill missing values\n    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n    \n    # Remove outliers\n    if remove_outliers:\n        outlier_mask = pd.Series(False, index=df_clean.index)\n        for col in numeric_cols:\n            outlier_mask |= detect_outliers(df_clean, col, method, threshold)\n        df_clean = df_clean[~outlier_mask]\n        print(f\"\ud83d\uddd1\ufe0f Removed {outlier_mask.sum()} outliers\")\n    \n    return df_clean\n\nprint(\"\u2705 Helper functions loaded\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \ud83d\udce1 Step 3: Load Data\n\nLoad the selected dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Load Data\n# ============================================================================\n\nif UPLOADED_DATA is not None:\n    data = UPLOADED_DATA\n    print(\"\u2705 Using uploaded data\")\nelif DATA_SOURCE_CHOICE == '1':\n    data = generate_environmental_data(30)\n    print(\"\u2705 Generated environmental data (720 readings)\")\nelif DATA_SOURCE_CHOICE == '2':\n    data = generate_soil_data(50)\n    print(\"\u2705 Generated soil data (50 samples)\")\nelif DATA_SOURCE_CHOICE == '3':\n    data = generate_plant_data(100)\n    print(\"\u2705 Generated plant growth data (100 plants)\")\nelse:\n    data = generate_environmental_data(30)\n    print(\"\u2705 Using default environmental data\")\n\nprint(f\"",
        "\ud83d\udcca Dataset shape: {data.shape}\")\nprint(f\"\ud83d\udccb Columns: {list(data.columns)}\")\n\n# Preview\ndisplay(Markdown(\"### \ud83d\udd0d Data Preview\"))\ndisplay(data.head(10))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Step 4: Clean and Analyze Data\n\nClean the data and compute statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Data Cleaning and Analysis\n# ============================================================================\n\nprint(\"\ud83d\udd04 Cleaning data...\")\ndata_clean = clean_data(data, REMOVE_OUTLIERS, OUTLIER_METHOD, Z_THRESHOLD)\n\nprint(f\"",
        "\ud83d\udcca Original: {len(data)} rows\")\nprint(f\"\ud83d\udcca Cleaned: {len(data_clean)} rows\")\n\n# Summary statistics\ndisplay(Markdown(\"### \ud83d\udcc8 Summary Statistics\"))\ndisplay(data_clean.describe())\n\n# Missing values\ndisplay(Markdown(\"### \ud83d\udd0d Missing Values\"))\nmissing = data.isnull().sum()\nif missing.sum() > 0:\n    display(missing[missing > 0])\nelse:\n    print(\"\u2705 No missing values\")\n\n# Data types\ndisplay(Markdown(\"### \ud83d\udccb Data Types\"))\ndisplay(pd.DataFrame({'Type': data_clean.dtypes, 'Count': data_clean.count()}))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Step 5: Visualizations\n\nCreate exploratory visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Data Visualization\n# ============================================================================\n\nnumeric_cols = data_clean.select_dtypes(include=[np.number]).columns\n\n# Distribution plots\nif len(numeric_cols) > 0:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n    \n    for i, col in enumerate(numeric_cols[:4]):\n        axes[i].hist(data_clean[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n        axes[i].set_title(f'Distribution: {col}', fontweight='bold')\n        axes[i].set_xlabel(col)\n        axes[i].set_ylabel('Frequency')\n        axes[i].grid(True, alpha=0.3)\n    \n    # Hide extra subplots\n    for i in range(len(numeric_cols), 4):\n        axes[i].set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Correlation heatmap\nif len(numeric_cols) > 1:\n    display(Markdown(\"### \ud83d\udd25 Correlation Heatmap\"))\n    plt.figure(figsize=(10, 8))\n    corr = data_clean[numeric_cols].corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f')\n    plt.title('Correlation Matrix', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n# Time series (if timestamp column exists)\ntime_col = [c for c in data_clean.columns if 'time' in c.lower() or 'date' in c.lower()]\nif time_col and len(numeric_cols) > 0:\n    display(Markdown(\"### \ud83d\udcc5 Time Series\"))\n    fig, ax = plt.subplots(figsize=(14, 6))\n    for col in numeric_cols[:3]:  # Plot first 3 numeric columns\n        ax.plot(data_clean[time_col[0]], data_clean[col], label=col, marker='o', markersize=2)\n    ax.set_xlabel('Time', fontsize=12)\n    ax.set_ylabel('Value', fontsize=12)\n    ax.set_title('Time Series Plot', fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\nprint(\"\u2705 Visualizations complete\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Step 6: Export and Citations\n\nExport cleaned data and document sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell",
      "metadata": {},
      "source": [
        "# ============================================================================\n# Export Results\n# ============================================================================\n\n# Export cleaned data\nexport_filename = f\"cleaned_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\ndata_clean.to_csv(export_filename, index=False)\nprint(f\"\u2705 Exported: {export_filename}\")\n\n# Summary report\ndisplay(Markdown(f\"\"\"\n### \ud83d\udccb Analysis Summary\n\n**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}  \n**Dataset:** {DATA_SOURCE_CHOICE}  \n**Original rows:** {len(data)}  \n**Cleaned rows:** {len(data_clean)}  \n**Columns:** {len(data_clean.columns)}  \n**Outlier method:** {OUTLIER_METHOD if REMOVE_OUTLIERS else 'None'}\n\n### \ud83d\udcda Data Sources\n- Sample data generated using NumPy (BSD License)\n- Analysis performed using Pandas and SciPy\n\n### \ud83d\udcd6 Citation\nIf using this notebook, please cite:\n> Botanical Colabs (2025). Horticultural Data Analysis & Exploration. \n> https://github.com/outobecca/botanical-colabs\n\n### \ud83d\udcdd Notes\n- Always verify results with domain experts\n- Sample data is for demonstration only\n- Clean uploaded data may have different characteristics\n\"\"\"))\n\nprint(\"",
        "\u2705 Analysis complete!\")\n"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}