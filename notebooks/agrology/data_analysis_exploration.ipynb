{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "[//]: # ( Horticultural Data Analysis and Exploration )\n[//]: # ( License: MIT License )\n[//]: # ( Repository: https://github.com/outobecca/botanical-colabs )\n\n# üìä Horticultural Data Analysis & Exploration\n**Version 1.0** | Created: 2025-11-04 | Author: Botanical Colabs Team\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/outobecca/botanical-colabs/blob/main/notebooks/data_analysis_exploration.ipynb)\n\n## üìã Overview\n\n**Purpose:** Load, clean, transform, and explore horticultural datasets to discover patterns and inform cultivation decisions.\n\n**Research Question:** How can we efficiently analyze sensor data, soil tests, and plant measurements to optimize growing conditions?\n\n### üéØ Use Cases\n- Load and clean environmental sensor data (temperature, humidity, light)\n- Analyze soil test results (pH, NPK nutrients, organic matter)\n- Explore plant growth measurements\n- Detect data anomalies\n- Generate summary statistics\n- Create visualizations\n\n### üìä Data Sources\n\n| Type | Format | Examples |\n|------|--------|----------|\n| **Sensors** | CSV, JSON | Temperature, humidity, light |\n| **Soil** | CSV, Excel | pH, NPK, moisture |\n| **Plants** | CSV, JSON | Height, yield, phenotype |\n| **Sample** | Built-in | Generated test data |\n\n### ‚ö†Ô∏è Notes\n- Sample data provided for learning\n- Upload CSV, Excel, or JSON files\n- Interactive forms for easy input\n- Export results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## üìö Background & Methodology\n\n### Scientific Context\nModern horticulture uses data from:\n- Environmental sensors (IoT devices, weather stations)\n- Soil laboratory analyses\n- Plant measurements (growth, yield)\n\nSystematic analysis helps optimize conditions and improve outcomes.\n\n### Methodology\n1. **Data Loading** - Import from files or generate samples\n2. **Data Cleaning** - Handle missing values and outliers\n3. **Exploratory Analysis** - Statistics and distributions\n4. **Visualization** - Charts and plots\n5. **Export** - Save results\n\n### Expected Outputs\n- Summary statistics\n- Time series plots\n- Distribution histograms\n- Correlation heatmaps\n- Cleaned datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Installation and Configuration\n\nRun the cells below to install libraries and configure your analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Library Installation and Import\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Installs required Python libraries.\n",
    "Run this cell first.\n",
    "\"\"\"\n",
    "\n",
    "# Installation\n",
    "!pip install -q pandas numpy matplotlib seaborn scipy ipywidgets openpyxl plotly scikit-learn\n",
    "\n",
    "# Core imports\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries installed successfully\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Interactive Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Data source selection (FORM)\n",
    "print(\"üìã SELECT DATA SOURCE:\")\n",
    "data_source_options = {\n",
    "    \"1\": \"Sample Environmental Data (30 days of sensor readings)\",\n",
    "    \"2\": \"Sample Soil Analysis Data (50 samples)\",\n",
    "    \"3\": \"Sample Plant Growth Data (100 plants)\",\n",
    "    \"4\": \"Upload My File (CSV, Excel, or JSON)\",\n",
    "}\n",
    "\n",
    "for key, desc in data_source_options.items():\n",
    "    print(f\"  [{key}] {desc}\")\n",
    "\n",
    "DATA_SOURCE_CHOICE = input(\"Enter choice (1-4): \").strip() or \"1\"\n",
    "\n",
    "# Upload file if needed\n",
    "UPLOADED_DATA = None\n",
    "if DATA_SOURCE_CHOICE == \"4\":\n",
    "    print(\"üì§ Upload your file in the next cell using Google Colab's file upload\")\n",
    "    from google.colab import files\n",
    "\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        print(f\"‚úÖ Uploaded: {filename}\")\n",
    "        # Load the file\n",
    "        if filename.endswith(\".csv\"):\n",
    "            UPLOADED_DATA = pd.read_csv(filename)\n",
    "        elif filename.endswith((\".xlsx\", \".xls\")):\n",
    "            UPLOADED_DATA = pd.read_excel(filename)\n",
    "        elif filename.endswith(\".json\"):\n",
    "            UPLOADED_DATA = pd.read_json(filename)\n",
    "        print(f\"üìä Loaded {len(UPLOADED_DATA)} rows\")\n",
    "\n",
    "# Outlier handling (FORM)\n",
    "print(\"üéØ OUTLIER DETECTION:\")\n",
    "print(\"  [1] Remove outliers (Z-score method)\")\n",
    "print(\"  [2] Remove outliers (IQR method)\")\n",
    "print(\"  [3] Keep all data\")\n",
    "\n",
    "OUTLIER_CHOICE = input(\"Enter choice (1-3): \").strip() or \"1\"\n",
    "REMOVE_OUTLIERS = OUTLIER_CHOICE in [\"1\", \"2\"]\n",
    "OUTLIER_METHOD = \"zscore\" if OUTLIER_CHOICE == \"1\" else \"iqr\"\n",
    "\n",
    "if REMOVE_OUTLIERS:\n",
    "    Z_THRESHOLD = float(input(\"Z-score threshold (default 3.0): \").strip() or \"3.0\")\n",
    "else:\n",
    "    Z_THRESHOLD = 3.0\n",
    "\n",
    "print(\"‚úÖ Configuration complete!\")\n",
    "print(f\"   Data source: {data_source_options[DATA_SOURCE_CHOICE]}\")\n",
    "print(f\"   Outlier handling: {OUTLIER_METHOD if REMOVE_OUTLIERS else 'None'}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## üîß Step 2: Helper Functions\n\nData processing utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def generate_environmental_data(days=30):\n",
    "    \"\"\"Generate sample environmental sensor data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(end=datetime.now(), periods=days * 24, freq=\"H\")\n",
    "    hours = np.array([d.hour for d in dates])\n",
    "\n",
    "    # Realistic patterns\n",
    "    temp = (\n",
    "        22 + 5 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 1, len(dates))\n",
    "    )\n",
    "    humidity = (\n",
    "        60 - 15 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 3, len(dates))\n",
    "    )\n",
    "    light = np.maximum(\n",
    "        0, 400 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 30, len(dates))\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp\": dates,\n",
    "            \"temperature_c\": temp,\n",
    "            \"humidity_percent\": humidity,\n",
    "            \"light_ppfd\": light,\n",
    "            \"soil_moisture\": 65 + np.random.normal(0, 5, len(dates)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add anomalies\n",
    "    anomalies = np.random.choice(len(df), 5, replace=False)\n",
    "    df.loc[anomalies, \"temperature_c\"] += np.random.choice([-10, 10], 5)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_soil_data(n=50):\n",
    "    \"\"\"Generate sample soil analysis data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"sample_id\": [f\"SOIL_{i:03d}\" for i in range(1, n + 1)],\n",
    "            \"location\": np.random.choice([\"Field A\", \"Field B\", \"Field C\"], n),\n",
    "            \"ph\": np.clip(np.random.normal(6.5, 0.5, n), 5.0, 8.0),\n",
    "            \"nitrogen_ppm\": np.clip(np.random.normal(45, 10, n), 0, 100),\n",
    "            \"phosphorus_ppm\": np.clip(np.random.normal(30, 8, n), 0, 80),\n",
    "            \"potassium_ppm\": np.clip(np.random.normal(180, 30, n), 0, 300),\n",
    "            \"organic_matter_%\": np.clip(np.random.normal(4.5, 1.2, n), 1, 10),\n",
    "            \"date\": pd.date_range(end=datetime.now(), periods=n),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_plant_data(n=100):\n",
    "    \"\"\"Generate sample plant growth data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    treatments = [\"Control\", \"Treatment A\", \"Treatment B\"]\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"plant_id\": [f\"P{i:04d}\" for i in range(1, n + 1)],\n",
    "            \"variety\": np.random.choice([\"Var1\", \"Var2\", \"Var3\"], n),\n",
    "            \"treatment\": np.random.choice(treatments, n),\n",
    "            \"height_cm\": np.random.normal(45, 12, n),\n",
    "            \"leaf_count\": np.random.poisson(25, n),\n",
    "            \"yield_g\": np.random.normal(125, 30, n),\n",
    "            \"date\": pd.date_range(end=datetime.now(), periods=n)[::-1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Treatment effects\n",
    "    for treat, factor in {\n",
    "        \"Control\": 1.0,\n",
    "        \"Treatment A\": 1.15,\n",
    "        \"Treatment B\": 1.25,\n",
    "    }.items():\n",
    "        mask = df[\"treatment\"] == treat\n",
    "        df.loc[mask, [\"height_cm\", \"yield_g\"]] *= factor\n",
    "\n",
    "    return df.clip(lower=0)\n",
    "\n",
    "\n",
    "def detect_outliers(df, column, method=\"zscore\", threshold=3.0):\n",
    "    \"\"\"Detect outliers in a column.\"\"\"\n",
    "    if method == \"zscore\":\n",
    "        z = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "        return z > threshold\n",
    "    else:  # IQR\n",
    "        Q1, Q3 = df[column].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        return (df[column] < Q1 - 1.5 * IQR) | (df[column] > Q3 + 1.5 * IQR)\n",
    "\n",
    "\n",
    "def clean_data(df, remove_outliers=True, method=\"zscore\", threshold=3.0):\n",
    "    \"\"\"Clean dataframe.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Fill missing values\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(\n",
    "        df_clean[numeric_cols].median()\n",
    "    )\n",
    "\n",
    "    # Remove outliers\n",
    "    if remove_outliers:\n",
    "        outlier_mask = pd.Series(False, index=df_clean.index)\n",
    "        for col in numeric_cols:\n",
    "            outlier_mask |= detect_outliers(df_clean, col, method, threshold)\n",
    "        df_clean = df_clean[~outlier_mask]\n",
    "        print(f\"üóëÔ∏è Removed {outlier_mask.sum()} outliers\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## üì° Step 3: Load Data\n\nLoad the selected dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Load Data\n",
    "# ============================================================================\n",
    "\n",
    "if UPLOADED_DATA is not None:\n",
    "    data = UPLOADED_DATA\n",
    "    print(\"‚úÖ Using uploaded data\")\n",
    "elif DATA_SOURCE_CHOICE == \"1\":\n",
    "    data = generate_environmental_data(30)\n",
    "    print(\"‚úÖ Generated environmental data (720 readings)\")\n",
    "elif DATA_SOURCE_CHOICE == \"2\":\n",
    "    data = generate_soil_data(50)\n",
    "    print(\"‚úÖ Generated soil data (50 samples)\")\n",
    "elif DATA_SOURCE_CHOICE == \"3\":\n",
    "    data = generate_plant_data(100)\n",
    "    print(\"‚úÖ Generated plant growth data (100 plants)\")\n",
    "else:\n",
    "    data = generate_environmental_data(30)\n",
    "    print(\"‚úÖ Using default environmental data\")\n",
    "\n",
    "print(f\"üìä Dataset shape: {data.shape}\")\n",
    "print(f\"üìã Columns: {list(data.columns)}\")\n",
    "\n",
    "# Preview\n",
    "display(Markdown(\"### üîç Data Preview\"))\n",
    "display(data.head(10))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Clean and Analyze Data\n\nClean the data and compute statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Data Cleaning and Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîÑ Cleaning data...\")\n",
    "data_clean = clean_data(data, REMOVE_OUTLIERS, OUTLIER_METHOD, Z_THRESHOLD)\n",
    "\n",
    "print(f\"üìä Original: {len(data)} rows\")\n",
    "print(f\"üìä Cleaned: {len(data_clean)} rows\")\n",
    "\n",
    "# Summary statistics\n",
    "display(Markdown(\"### üìà Summary Statistics\"))\n",
    "display(data_clean.describe())\n",
    "\n",
    "# Missing values\n",
    "display(Markdown(\"### üîç Missing Values\"))\n",
    "missing = data.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    display(missing[missing > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values\")\n",
    "\n",
    "# Data types\n",
    "display(Markdown(\"### üìã Data Types\"))\n",
    "display(pd.DataFrame({\"Type\": data_clean.dtypes, \"Count\": data_clean.count()}))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## üìä Step 5: Visualizations\n\nCreate exploratory visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Data Visualization\n",
    "# ============================================================================\n",
    "\n",
    "numeric_cols = data_clean.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Distribution plots\n",
    "if len(numeric_cols) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(numeric_cols[:4]):\n",
    "        axes[i].hist(data_clean[col].dropna(), bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "        axes[i].set_title(f\"Distribution: {col}\", fontweight=\"bold\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(\"Frequency\")\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide extra subplots\n",
    "    for i in range(len(numeric_cols), 4):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "if len(numeric_cols) > 1:\n",
    "    display(Markdown(\"### üî• Correlation Heatmap\"))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr = data_clean[numeric_cols].corr()\n",
    "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0, square=True, fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Time series (if timestamp column exists)\n",
    "time_col = [c for c in data_clean.columns if \"time\" in c.lower() or \"date\" in c.lower()]\n",
    "if time_col and len(numeric_cols) > 0:\n",
    "    display(Markdown(\"### üìÖ Time Series\"))\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for col in numeric_cols[:3]:  # Plot first 3 numeric columns\n",
    "        ax.plot(\n",
    "            data_clean[time_col[0]],\n",
    "            data_clean[col],\n",
    "            label=col,\n",
    "            marker=\"o\",\n",
    "            markersize=2,\n",
    "        )\n",
    "    ax.set_xlabel(\"Time\", fontsize=12)\n",
    "    ax.set_ylabel(\"Value\", fontsize=12)\n",
    "    ax.set_title(\"Time Series Plot\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations complete\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell",
   "metadata": {},
   "source": [
    "## üìö Step 6: Export and Citations\n\nExport cleaned data and document sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Export Results\n",
    "# ============================================================================\n",
    "\n",
    "# Export cleaned data\n",
    "export_filename = f\"cleaned_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "data_clean.to_csv(export_filename, index=False)\n",
    "print(f\"‚úÖ Exported: {export_filename}\")\n",
    "\n",
    "# Summary report\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "### üìã Analysis Summary\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}  \n",
    "**Dataset:** {DATA_SOURCE_CHOICE}  \n",
    "**Original rows:** {len(data)}  \n",
    "**Cleaned rows:** {len(data_clean)}  \n",
    "**Columns:** {len(data_clean.columns)}  \n",
    "**Outlier method:** {OUTLIER_METHOD if REMOVE_OUTLIERS else 'None'}\n",
    "\n",
    "### üìö Data Sources\n",
    "- Sample data generated using NumPy (BSD License)\n",
    "- Analysis performed using Pandas and SciPy\n",
    "\n",
    "### üìñ Citation\n",
    "If using this notebook, please cite:\n",
    "> Botanical Colabs (2025). Horticultural Data Analysis & Exploration. \n",
    "> https://github.com/outobecca/botanical-colabs\n",
    "\n",
    "### üìù Notes\n",
    "- Always verify results with domain experts\n",
    "- Sample data is for demonstration only\n",
    "- Clean uploaded data may have different characteristics\n",
    "\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
